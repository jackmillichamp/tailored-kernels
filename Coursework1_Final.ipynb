{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coursework 1\n",
    "\n",
    "Completing two classification tasks. One of the classification tasks is related to image classification and the other relates to text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "\n",
    "In this task, you are provided with three classes of noisy images, cars, bikes and people in real world settings. You need to implement a boosting based classifier that can be used to classify the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For image classification...\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import cv2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# For sentiment analysis...\n",
    "import string\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('popular')\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import LancasterStemmer\n",
    "lanc = LancasterStemmer()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_image(image, angle):\n",
    "    return ndimage.rotate(image, angle, reshape=False, mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_image(image, shift):\n",
    "    '''Shift is either a float or sequence of 2 floats, one for each axis'''\n",
    "    return ndimage.shift(image, shift, mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_brightness(image, value):\n",
    "    value = np.int(value)\n",
    "    if value > 0:\n",
    "        new_image = np.where((255 - image) < value, 255, image+value)\n",
    "    else:\n",
    "        value = np.abs(value)\n",
    "        new_image = np.where((image < value), 0, image-value)\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hflip_image(image):\n",
    "    '''Flips image horizontally'''\n",
    "    return np.fliplr(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image, n_out=10, shift_range=[-15,15], rot_range=[-15,15], hflip=True, brightness_range=[-25,25]):\n",
    "    '''n_out = Number of output images (excluding original)'''\n",
    "    \n",
    "    # Randomising augmentation parameters...\n",
    "    augmented_images = []\n",
    "    rand_angles = np.random.rand(n_out)*(rot_range[1]-rot_range[0]) + rot_range[0]\n",
    "    rand_shifts = np.random.rand(n_out)*(shift_range[1]-shift_range[0]) + shift_range[0]\n",
    "    rand_brightness = np.random.rand(n_out)*(brightness_range[1]-brightness_range[0]) + brightness_range[0]\n",
    "    if hflip == True:\n",
    "        rand_flips = np.random.choice([True, False], size=n_out)\n",
    "    else:\n",
    "        rand_flips = [False]*n_out\n",
    "    \n",
    "    # Creating n augmented images from input image...\n",
    "    for i in range(n_out):\n",
    "        augmented_image = rotate_image(image, rand_angles[i])\n",
    "        augmented_image = shift_image(augmented_image, rand_shifts[i])\n",
    "        augmented_image = change_brightness(augmented_image, rand_brightness[i])\n",
    "        if rand_flips[i] == True:\n",
    "            augmented_image = hflip_image(augmented_image)\n",
    "        \n",
    "        # Adding image to output array...\n",
    "        augmented_images.append(augmented_image)\n",
    "    \n",
    "    return augmented_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool(image, KL=(2,2)):\n",
    "    M, N = image.shape\n",
    "    (K, L) = KL\n",
    "\n",
    "    MK = M // K\n",
    "    NL = N // L\n",
    "    return image[:MK*K, :NL*L].reshape(MK, K, NL, L).max(axis=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_dataset_train_test(folder_name_train, folder_name_test, n_aug=1, hflip=False, pool=True, combined=True):    \n",
    "    ## Processing train images first, NOT including test images in this stage\n",
    "    ## Processing training images...\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for fullpath in glob.iglob(f'{folder_name_train}/*/*', recursive = True):\n",
    "        _, target, file = fullpath.split(os.path.sep)\n",
    "        \n",
    "        # Getting greyscaled image data...\n",
    "        if pool == True:\n",
    "            X_train.append(max_pool(cv2.imread(fullpath, 0), (1,2)))\n",
    "        else:\n",
    "            X_train.append(cv2.imread(fullpath, 0))\n",
    "        y_train.append(target)\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    \n",
    "    # Initialising hog converter...\n",
    "    hog_feature_len=34020\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    \n",
    "    # Converting y to 1, 2, 3 format...\n",
    "    Xtrain = []\n",
    "    ytrain = []\n",
    "    for i, observation in enumerate(y_train):\n",
    "        if observation == 'cars':\n",
    "            y_train[i] = 1\n",
    "        elif observation == 'bikes':\n",
    "            y_train[i] = 2\n",
    "        else:\n",
    "            y_train[i] = 3\n",
    "        \n",
    "        # Augmenting training data...\n",
    "        image = X_train[i]\n",
    "        if combined == True:\n",
    "            Xtrain.append(np.append(image.reshape(-1), hog.compute(image).reshape(-1)))\n",
    "        else:\n",
    "            Xtrain.append(image.reshape(-1))\n",
    "        ytrain.append(y_train[i])\n",
    "        \n",
    "        # Flipping training images...\n",
    "        image_flipped = np.fliplr(image)\n",
    "        if combined == True:\n",
    "            Xtrain.append(np.append(image_flipped.reshape(-1), hog.compute(image_flipped).reshape(-1)))\n",
    "        else:\n",
    "            Xtrain.append(image_flipped.reshape(-1))\n",
    "        ytrain.append(y_train[i])\n",
    "        #'''\n",
    "        # Augmenting training images, creating n_aug new images for each...\n",
    "        image = X_train[i]\n",
    "        augmented_images = augment_image(image, n_out=n_aug, hflip=True)\n",
    "        for augmented_img in augmented_images:\n",
    "            if combined == True:\n",
    "                Xtrain.append(np.append(augmented_img.reshape(-1), hog.compute(augmented_img).reshape(-1)))\n",
    "            else:\n",
    "                Xtrain.append(augmented_img.reshape(-1))\n",
    "            ytrain.append(y_train[i])\n",
    "      \n",
    "    X_train = np.array(Xtrain)\n",
    "    y_train = np.array(ytrain).astype(int)\n",
    "    \n",
    "    # Shuffling training images...\n",
    "    shuffle_inds = np.arange(y_train.size)\n",
    "    np.random.shuffle(shuffle_inds)\n",
    "    X_train = X_train[shuffle_inds]\n",
    "    y_train = y_train[shuffle_inds]\n",
    "    \n",
    "    # Standardising (fitting on training data only)...\n",
    "    standariser = StandardScaler()\n",
    "    X_train = standariser.fit_transform(X_train)\n",
    "    \n",
    "    ## Processing test images over here, while ensuring that there is no data leakage from train to test\n",
    "    ## Processing test images...\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for fullpath in glob.iglob(f'{folder_name_test}/*/*', recursive = True):\n",
    "        _, target, file = fullpath.split(os.path.sep)\n",
    "        \n",
    "        # Getting greyscaled image data...\n",
    "        if pool == True:\n",
    "            test_image = max_pool(cv2.imread(fullpath, 0), (1,2))\n",
    "        else:\n",
    "            test_image = cv2.imread(fullpath, 0)\n",
    "        if combined == True:\n",
    "            X_test.append(np.append(test_image.reshape(-1), hog.compute(test_image).reshape(-1)))\n",
    "        else:\n",
    "            X_test.append(test_image.reshape(-1))\n",
    "        y_test.append(target)\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    # Converting y to -1, 0, 1 format...\n",
    "    for i, observation in enumerate(y_test):\n",
    "        if observation == 'cars':\n",
    "            y_test[i] = 1\n",
    "        elif observation == 'bikes':\n",
    "            y_test[i] = 2\n",
    "        else:\n",
    "            y_test[i] = 3\n",
    "        \n",
    "    y_test = y_test.astype(int)\n",
    "    \n",
    "    # Standardising (fit on training data only)...\n",
    "    X_test = standariser.transform(X_test)\n",
    "    \n",
    "    return (X_train, y_train, X_test, y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostingClassifier:\n",
    "    \n",
    "    '''Adaboost classifier'''\n",
    "    \n",
    "    def __init__(self, n_its=135, max_depth=2, print_progress=False):\n",
    "        self.n_its = n_its\n",
    "        self.model_weights = np.zeros(n_its)\n",
    "        self.models = np.zeros(shape=n_its, dtype=object)\n",
    "        self.print_progress = print_progress\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Initialising the weights...\n",
    "        self.n_classes = np.unique(y).size\n",
    "        n_obs = y.size\n",
    "        weights = np.ones(n_obs) * (1 / n_obs)\n",
    "        \n",
    "        # Applying boosting iterations...\n",
    "        for itr in range(self.n_its):\n",
    "            if self.print_progress == True:\n",
    "                print('Iteration', itr+1)\n",
    "            \n",
    "            # Implimenting decision stump...\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "            tree = tree.fit(X, y, sample_weight=weights)\n",
    "            pred_y = tree.predict(X)\n",
    "            \n",
    "            \n",
    "            # Calculating weighted error...\n",
    "            mod = np.sum(np.abs(weights))\n",
    "            diff = pred_y - y\n",
    "            loss = np.array([1 if i!=0 else 0 for i in diff])\n",
    "            error = (1 / mod) * np.dot(weights, loss)\n",
    "            \n",
    "            # Model weight...\n",
    "            model_weight = np.log((1 - error) / error) + np.log(self.n_classes - 1)\n",
    "            self.model_weights[itr] = model_weight\n",
    "            \n",
    "            # Updated weights...\n",
    "            weights *= np.exp((model_weight) * loss)\n",
    "            weights /= np.sum(np.abs(weights))\n",
    "            \n",
    "            # Add tree to list...\n",
    "            self.models[itr] = tree\n",
    "        \n",
    "        if self.print_progress == True:\n",
    "            print('Model fit!')\n",
    "        return self\n",
    "\n",
    "    def boost_predict(self, X):\n",
    "        \n",
    "        model_weights = self.model_weights\n",
    "        n_obs = X.shape[0]\n",
    "        \n",
    "        # Prediction of boosting classifier...\n",
    "        #model_predictions = np.zeros((self.n_its, n_obs), dtype=object)\n",
    "        model_predictions = []\n",
    "        \n",
    "        for itr, model in enumerate(self.models):\n",
    "            #model_predictions[itr] = model.predict(X)\n",
    "            model_predictions.append(model.predict(X))\n",
    "        \n",
    "        model_predictions = np.array(model_predictions)\n",
    "        \n",
    "        predictions = np.zeros(n_obs)\n",
    "        n_models = model_weights.size\n",
    "\n",
    "        for col_num in range(n_obs):\n",
    "            weighted_counts = np.zeros(self.n_classes)\n",
    "            for model_num in range(n_models):\n",
    "                pred = model_predictions[model_num, col_num] - 1\n",
    "                weighted_counts[pred] += model_weights[model_num]\n",
    "            if self.n_classes == 2:\n",
    "                predictions[col_num] = np.argmin(weighted_counts)\n",
    "            else:\n",
    "                predictions[col_num] = np.argmax(weighted_counts) + 1\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(model, X, y, k_splits=5, boost=True, seed=None, text=False):\n",
    "    N = y.size\n",
    "    split_size = round(N / k_splits)\n",
    "    shuffle = np.arange(N)\n",
    "    if seed is None:\n",
    "        np.random.shuffle(shuffle)\n",
    "    else:\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(shuffle)\n",
    "    \n",
    "    X = X[shuffle]\n",
    "    y = y[shuffle]\n",
    "    \n",
    "    test_accs = []\n",
    "    train_accs = []\n",
    "    \n",
    "    for split in range(k_splits):\n",
    "        print('Split', split+1, 'in progress')\n",
    "        \n",
    "        # Deciding on test/train splits...\n",
    "        if split + 1 < k_splits:\n",
    "            if text == True:\n",
    "                X_test = X[split*split_size:(split+1)*split_size]\n",
    "                y_test = y[split*split_size:(split+1)*split_size]\n",
    "                X_train = np.append(X[:split*split_size], X[(split+1)*split_size:])\n",
    "                y_train = np.append(y[:split*split_size], y[(split+1)*split_size:])\n",
    "            else:\n",
    "                X_test = X[split*split_size:(split+1)*split_size,:]\n",
    "                y_test = y[split*split_size:(split+1)*split_size]\n",
    "                X_train = np.vstack((X[:split*split_size,:], X[(split+1)*split_size:,:]))\n",
    "                y_train = np.append(y[:split*split_size], y[(split+1)*split_size:])\n",
    "        else:\n",
    "            if text == True:\n",
    "                X_test = X[split*split_size:]\n",
    "                y_test = y[split*split_size:]\n",
    "                X_train = X[:split*split_size]\n",
    "                y_train = y[:split*split_size]\n",
    "            else:\n",
    "                X_test = X[split*split_size:,:]\n",
    "                y_test = y[split*split_size:]\n",
    "                X_train = X[:split*split_size,:]\n",
    "                y_train = y[:split*split_size]\n",
    "        \n",
    "        if text == True:\n",
    "            pt = process_text()\n",
    "            # Vectorising train and converting to tf-idf format (fitting to train data only)...\n",
    "            X_train = pt.fit_transform(X_train)\n",
    "            # Vectorising test and converting to tf-idf format (fitting to train data only)...\n",
    "            X_test = pt.transform(X_test)\n",
    "            \n",
    "            if boost == True:\n",
    "                # Fitting model to text data via boosting...\n",
    "                model.fit(X_train, y_train)\n",
    "                # Predcting test values...\n",
    "                y_test_pred = model.boost_predict(X_test)\n",
    "            \n",
    "            else:\n",
    "                # Fitting model to processed text data via SVM...\n",
    "                model.fit_text(X_train, y_train)\n",
    "                # Predcting test values...\n",
    "                y_test_pred = model.predict_text(X_test)\n",
    "            \n",
    "        elif boost == True:\n",
    "            # Fitting model to image data via boosting...\n",
    "            model.fit(X_train, y_train)\n",
    "            # Predcting test values...\n",
    "            y_test_pred = model.boost_predict(X_test)\n",
    "        \n",
    "        else:\n",
    "            # Fitting model to image data via SVM...\n",
    "            model.fit_image(X_train, y_train)\n",
    "            # Predcting test values...\n",
    "            y_test_pred = model.predict_image(X_test)\n",
    "        \n",
    "        \n",
    "        # Calculating prediction accuracy...\n",
    "        test_acc = accuracy_score(y_test, y_test_pred)\n",
    "        \n",
    "        # Predcting train values...\n",
    "        if boost == True:\n",
    "            y_train_pred = model.boost_predict(X_train)\n",
    "        else:\n",
    "            y_train_pred = model.predict_image(X_train)\n",
    "        \n",
    "        # Calculating prediction accuracy...\n",
    "        train_acc = accuracy_score(y_train, y_train_pred)\n",
    "        \n",
    "        # Adding accuracies to list...\n",
    "        test_accs.append(test_acc)\n",
    "        train_accs.append(train_acc)\n",
    "    \n",
    "    return test_accs, train_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_n_augs = np.arange(6)\n",
    "all_its = [5,7,9]\n",
    "\n",
    "test_cv_means = []\n",
    "train_cv_means = []\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "for i, n_aug in enumerate(all_n_augs):\n",
    "    train_it_mean = []\n",
    "    test_it_mean = []\n",
    "    for j, n_its in enumerate(all_its):\n",
    "        print('\\n\\nIteration number {}:   ({} aug images, {} learners)'.format(i*3 + j, n_aug, n_its))\n",
    "        Xtrain, ytrain, Xtest, ytest = process_images(n_aug=n_aug)\n",
    "        bc = BoostingClassifier(n_its, print_progress=False)\n",
    "        test_score, train_score = cross_validate(bc, Xtrain, ytrain, seed=2)\n",
    "        train_it_mean.append(np.mean(test_score))\n",
    "        test_it_mean.append(np.mean(train_score))\n",
    "        print('Test Scores:\\n', test_score, np.mean(test_score))\n",
    "        print('\\nTrain Scores:\\n', train_score, np.mean(train_score))\n",
    "    test_cv_means.append(test_it_mean)\n",
    "    train_cv_means.append(train_it_mean)\n",
    "\n",
    "# Printing time taken...\n",
    "t2 = time.time()\n",
    "delta_t = t2-t1\n",
    "mins = delta_t//60\n",
    "secs = delta_t - (mins*60)\n",
    "print('\\n{} mins and {} seconds'.format(mins, secs))\n",
    "\n",
    "# Plotting the graph...\n",
    "fig = plt.figure(figsize=(16,10))\n",
    "plt.title('Mean accuracy of 5-fold cross validation vs. number of learners', fontsize=20, pad=10)\n",
    "plt.ylabel('Mean accuracy', fontsize=20, labelpad=15)\n",
    "plt.xlabel('Number of boosting learners', fontsize=20, labelpad=15)\n",
    "#plt.plot(rand_its, test_cv_means, label='Test', color='r')\n",
    "#plt.plot(rand_its, train_cv_means, label='Train', color='b')\n",
    "plt.plot(all_n_augs, test_cv_means, label='Test', color='r')\n",
    "plt.plot(all_n_augs, train_cv_means, label='Train', color='b')\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.savefig('n_augmented_boost.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "\n",
    "Classifing the above dataset using a Support Vector Machine (SVM) with tailored kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMClassifier:\n",
    "    def __init__(self, kernel='rbf'):\n",
    "        # Implement initialisation...\n",
    "        self.some_paramter=1\n",
    "        self.kernel = kernel\n",
    "        self.svc = OneVsRestClassifier(SVC(kernel=self.kernel))\n",
    "        self.svc_text = SVC(kernel=self.kernel)\n",
    "        \n",
    "    def fit_image(self, X,y):        \n",
    "        #training of the SVM\n",
    "        self.svc.fit(X, y)\n",
    "        # providing for separate image kernels\n",
    "        return\n",
    "    \n",
    "    def fit_text(self, X,y):\n",
    "        # Training of the SVM\n",
    "        self.svc_text.fit(X, y)\n",
    "        # providing for separate text kernels\n",
    "        return\n",
    "    \n",
    "    def predict_image(self, X):\n",
    "        # prediction routine for the SVM\n",
    "        pred_y = self.svc.predict(X)\n",
    "        return pred_y\n",
    "    \n",
    "    def predict_text(self, X):\n",
    "        # prediction routine for the SVM\n",
    "        pred_y = self.svc_text.predict(X)\n",
    "        return pred_y  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gaussian_kernel():\n",
    "    '''\n",
    "    Defining the gaussian kernel for a given sigma.\n",
    "    (VECTORISED)\n",
    "    ========================================\n",
    "    Input\n",
    "    ========================================\n",
    "    X1: 2D numpy array\n",
    "    X2: 2D numpy array\n",
    "    sigma: A hyper parameter\n",
    "    \n",
    "    ========================================\n",
    "    Output\n",
    "    ========================================\n",
    "    Covariance matrix (2D numpy array)\n",
    "    '''\n",
    "    def __init__(self, sigma=140):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def fit(self, X1, X2):\n",
    "        X1_norm = np.square(np.linalg.norm(X1, axis=1))\n",
    "        X2_norm = np.square(np.linalg.norm(X2, axis=1))\n",
    "        \n",
    "        distances = X1_norm.reshape(-1, 1) + X2_norm.reshape(1, -1) - 2*np.dot(X1, X2.T)\n",
    "\n",
    "        arg = distances / (2*(self.sigma**2))\n",
    "        return np.exp(-arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class log_kernel():\n",
    "    '''\n",
    "    Defining the log kernel for a given d.\n",
    "    (VECTORISED)\n",
    "    ========================================\n",
    "    Input\n",
    "    ========================================\n",
    "    X1: 2D numpy array\n",
    "    X2: 2D numpy array\n",
    "    d: A hyper parameter\n",
    "    \n",
    "    ========================================\n",
    "    Output\n",
    "    ========================================\n",
    "    Covariance matrix (2D numpy array)\n",
    "    '''\n",
    "    def __init__(self, d=1):\n",
    "        self.d = d\n",
    "    \n",
    "    def fit(self, X1, X2):\n",
    "        X1_norm = np.square(np.linalg.norm(X1, axis=1))\n",
    "        X2_norm = np.square(np.linalg.norm(X2, axis=1))\n",
    "        \n",
    "        distances = X1_norm.reshape(-1, 1) + X2_norm.reshape(1, -1) - 2*np.dot(X1, X2.T)\n",
    "\n",
    "        arg = distances**self.d + 1\n",
    "        return - np.log(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class students_t_kernel():\n",
    "    '''\n",
    "    Defining the student's t kernel for a given d.\n",
    "    (VECTORISED)\n",
    "    ========================================\n",
    "    Input\n",
    "    ========================================\n",
    "    X1: 2D numpy array\n",
    "    X2: 2D numpy array\n",
    "    d: A hyper parameter\n",
    "    \n",
    "    ========================================\n",
    "    Output\n",
    "    ========================================\n",
    "    Covariance matrix (2D numpy array)\n",
    "    '''\n",
    "    def __init__(self, d=1):\n",
    "        self.d = d\n",
    "    \n",
    "    def fit(self, X1, X2):\n",
    "        X1_norm = np.square(np.linalg.norm(X1, axis=1))\n",
    "        X2_norm = np.square(np.linalg.norm(X2, axis=1))\n",
    "        \n",
    "        distances = X1_norm.reshape(-1, 1) + X2_norm.reshape(1, -1) - 2*np.dot(X1, X2.T)\n",
    "\n",
    "        arg = distances**self.d + 1\n",
    "        return 1 / arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cauchy_kernel():\n",
    "    '''\n",
    "    Defining the student's t kernel for a given d.\n",
    "    (VECTORISED)\n",
    "    ========================================\n",
    "    Input\n",
    "    ========================================\n",
    "    X1: 2D numpy array\n",
    "    X2: 2D numpy array\n",
    "    d: A hyper parameter\n",
    "    \n",
    "    ========================================\n",
    "    Output\n",
    "    ========================================\n",
    "    Covariance matrix (2D numpy array)\n",
    "    '''\n",
    "    def __init__(self, sigma=1):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def fit(self, X1, X2):\n",
    "        X1_norm = np.square(np.linalg.norm(X1, axis=1))\n",
    "        X2_norm = np.square(np.linalg.norm(X2, axis=1))\n",
    "        \n",
    "        distances = X1_norm.reshape(-1, 1) + X2_norm.reshape(1, -1) - 2*np.dot(X1, X2.T)\n",
    "\n",
    "        arg = (distances / (self.sigma**2)) + 1\n",
    "        return 1 / arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tanh_kernel():\n",
    "    '''\n",
    "    Defining the hyperbolic tangent kernel for a given k and c.\n",
    "    (VECTORISED)\n",
    "    ========================================\n",
    "    Input\n",
    "    ========================================\n",
    "    X1: 2D numpy array\n",
    "    X2: 2D numpy array\n",
    "    k: Scale hyper parameter\n",
    "    c: Offset hyper parameter\n",
    "    \n",
    "    ========================================\n",
    "    Output\n",
    "    ========================================\n",
    "    Covariance matrix (2D numpy array)\n",
    "    '''\n",
    "    def __init__(self, k=1, c=0):\n",
    "        self.k = k\n",
    "        self.c = c\n",
    "    \n",
    "    def fit(self, X1, X2):\n",
    "        return np.tanh((self.k * X1 @ X2.T) + self.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class histogram_intersection_kernel():\n",
    "    '''\n",
    "    Defining the histogram intersection kernel.\n",
    "    (VECTORISED)\n",
    "    ========================================\n",
    "    Input\n",
    "    ========================================\n",
    "    X1: 2D numpy array\n",
    "    X2: 2D numpy array\n",
    "    \n",
    "    ========================================\n",
    "    Output\n",
    "    ========================================\n",
    "    Covariance matrix (2D numpy array)\n",
    "    '''\n",
    "    def fit(self, X1, X2):\n",
    "        N1 = X1.shape[0]\n",
    "        N2 = X2.shape[0]\n",
    "        \n",
    "        cov = np.zeros((N1,N2))\n",
    "        for i in range(N1):\n",
    "            for j in range(N2):\n",
    "                cov[i,j] = np.sum(np.minimum(X1[i], X2[j]))\n",
    "\n",
    "        return cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing kernels..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = Xtrain.shape[1]\n",
    "var = np.var(Xtrain)\n",
    "sigma = np.sqrt(0.5 * M * var)\n",
    "\n",
    "#'linear'                            # It's ok\n",
    "gaus = gaussian_kernel(sigma=sigma) # ls = 140   # Pretty good\n",
    "tanh = tanh_kernel(k=sigma, c=0)     # This is bad, sk's sigmoid is better thoughsvc = OneVsRestClassifier(SVC(kernel=cauchy.fit))\n",
    "log = log_kernel(d=2)                # Great, no optimal hyperparamter though!\n",
    "hist = histogram_intersection_kernel()  # Great and no hyper parameters!\n",
    "stud = students_t_kernel(d=1)        # Bad, massively overfits\n",
    "cauchy = cauchy_kernel(sigma=sigma) # Great, doesn't overfit as much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#svc = OneVsRestClassifier(SVC(kernel=cauchy.fit))\\nsvc = SVMClassifier(kernel=cauchy.fit)\\ntest_accs, train_accs = cross_validate(svc, Xtrain, ytrain, boost=False, seed=None)\\n\\nprint('')\\nprint(test_accs, np.mean(test_accs), '\\n')\\nprint(train_accs, np.mean(train_accs))\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVMClassifier(kernel=cauchy.fit)\n",
    "test_accs, train_accs = cross_validate(svc, Xtrain, ytrain, boost=False, seed=None)\n",
    "\n",
    "print('')\n",
    "print(test_accs, np.mean(test_accs), '\\n')\n",
    "print(train_accs, np.mean(train_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySVC = SVMClassifier(kernel=hist.fit)\n",
    "mySVC.fit_image(Xtrain, ytrain)\n",
    "y_pred = mySVC.predict_image(Xtest)\n",
    "print('accuracy', accuracy_score(ytest, y_pred))\n",
    "trainy_pred = mySVC.predict_image(Xtrain)\n",
    "print('accuracy', accuracy_score(ytrain, trainy_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nlss = np.linspace(0.01, 7, 250)\\nsf = 1\\nbest_acc = 0\\nfor i, ls in enumerate(lss):\\n    print('\\nIter:', i+1)\\n    print('===========')\\n    print('Length-scale = {}, sigma_f = {}\\n'.format(ls,sf))\\n    g = g_kernel(length_scale=ls, sigma_f=sf)\\n    sc = OneVsRestClassifier(SVC(kernel=g.out))\\n    sc.fit(Xtrain, ytrain)\\n    y_pred = sc.predict(Xtest)\\n    acc = accuracy_score(ytest, y_pred)\\n    print('accuracy', acc)\\n    if acc > best_acc:\\n        best_acc = acc\\n        best_ls = ls\\n        best_sf = sf\\n        print('New Best!')\\nprint('\\n\\nOverall best acc =', best_acc)\\nprint(best_ls, best_sf)\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lss = np.linspace(0.01, 7, 250)\n",
    "sf = 1\n",
    "best_acc = 0\n",
    "for i, ls in enumerate(lss):\n",
    "    print('\\nIter:', i+1)\n",
    "    print('===========')\n",
    "    print('Length-scale = {}, sigma_f = {}\\n'.format(ls,sf))\n",
    "    g = g_kernel(length_scale=ls, sigma_f=sf)\n",
    "    sc = OneVsRestClassifier(SVC(kernel=g.out))\n",
    "    sc.fit(Xtrain, ytrain)\n",
    "    y_pred = sc.predict(Xtest)\n",
    "    acc = accuracy_score(ytest, y_pred)\n",
    "    print('accuracy', acc)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_ls = ls\n",
    "        best_sf = sf\n",
    "        print('New Best!')\n",
    "print('\\n\\nOverall best acc =', best_acc)\n",
    "print(best_ls, best_sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "\n",
    "Obtaining sentiment analysis for the movie review dataset. The dataset consists of movie reviews with the sentiments being provided. The sentiments are either positive or negative. I will train a boosting-based classifier to obtain train and cross-validate on the dataset provided. The method will be evaluated against an external test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the text and obtain a bag of words-based features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reviews(filename):\n",
    "    X = []\n",
    "    y = []\n",
    "    with open(filename, 'r', newline='', encoding='Latin1') as file:  # \n",
    "            reviews = csv.reader(file)\n",
    "            next(reviews, None)\n",
    "            # Reading in and appending reviews to raw_x list...\n",
    "            for review in reviews:\n",
    "                X.append(review[0])\n",
    "\n",
    "                # Converting 'positive' to a y-value of 1 and 'negative' to a y-value of 0...\n",
    "                # Appending this to raw_y list...\n",
    "                if review[1] == 'positive':\n",
    "                    y.append(1)\n",
    "                else:\n",
    "                    y.append(0)\n",
    "    \n",
    "    # Converting to numpy arrays...\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stop-words and punctuation, and to stem words...\n",
    "def clean_text(reviews):   \n",
    "    stop_words = set(stopwords.words('english'))   # set(c_stop_words) | set(stopwords.words('english'))\n",
    "    stp_wrds_punc = stop_words | set(string.punctuation)\n",
    "    \n",
    "    sep = ' '\n",
    "    for i, review in enumerate(reviews):\n",
    "        # Converting to lowercase and temporarlity tokenising words...\n",
    "        tokens = word_tokenize(review.lower())\n",
    "        # Removing stop-words, punctuation, numbers and stemming words...\n",
    "        fltd_tokens = [lanc.stem(token) for token in tokens if token not in stp_wrds_punc and token.isalpha()]\n",
    "        reviews[i] = sep.join(fltd_tokens)\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bag_of_words(train_file):\n",
    "    # Extracting training reveiws from CSV file...\n",
    "    X, y = extract_reviews(train_file)\n",
    "\n",
    "    # Randomising reviews...\n",
    "    np.random.seed(5)\n",
    "    shuffle = np.random.permutation(X.shape[0])\n",
    "    X = X[shuffle]\n",
    "    y = y[shuffle]\n",
    "    \n",
    "    # Train/Test split...\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size=0.8, random_state=123)\n",
    "    \n",
    "    # Removing stop-words, punctuation and stem words...\n",
    "    Xtrain = clean_text(Xtrain)\n",
    "    Xtest = clean_text(Xtest)\n",
    "    \n",
    "    return (Xtrain, ytrain, Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorising with bi-grams and applying tf-idf...\n",
    "class process_text():\n",
    "    def __init__(self, sparse=True):\n",
    "        self.sparse = sparse\n",
    "        self.tfvec = TfidfVectorizer(ngram_range=(1,2))#, max_features=10000)\n",
    "        \n",
    "    def fit_transform(self, X):\n",
    "        # Vectorising with bi-grams...\n",
    "        X = self.tfvec.fit_transform(X)\n",
    "        if self.sparse == True:\n",
    "            return X\n",
    "        else:\n",
    "            return X.toarray()\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Vectorising with bi-grams...\n",
    "        X = self.tfvec.transform(X)\n",
    "        if self.sparse == True:\n",
    "            return X\n",
    "        else:\n",
    "            return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bag_of_words_train_test(train_file, test_file, sparse=True):\n",
    "    # Process training data first and ensure the test data is not used while extracting bag of words feature vector\n",
    "    # Extracting training reveiws from CSV file...\n",
    "    Xtrain, ytrain = extract_reviews(train_file)\n",
    "    \n",
    "    # Removing stop-words, punctuation and stem words...\n",
    "    Xtrain = clean_text(Xtrain)\n",
    "    \n",
    "    # Vectorising and converting to tf-idf format (fitting to train data only)...\n",
    "    pt = process_text(sparse=sparse)\n",
    "    Xtrain = pt.fit_transform(Xtrain)\n",
    "    \n",
    "    \n",
    "    ## Process testing data here. Ensure that test data is not used above\n",
    "    # Extracting test reveiws from CSV file...\n",
    "    Xtest, ytest = extract_reviews(test_file)\n",
    "    \n",
    "    # Removing stop-words, punctuation and stem words...\n",
    "    Xtest = clean_text(Xtest)\n",
    "    \n",
    "    # Vectorising and converting to tf-idf format (fitting to train data only)...\n",
    "    Xtest = pt.transform(Xtest)\n",
    "    \n",
    "    return (Xtrain, ytrain, Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation on training set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting and cleaning review data...\n",
    "Xtrain, ytrain, Xtest, ytest = extract_bag_of_words('movie_review_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbc = BoostingClassifier(n_its=100, max_depth=2, print_progress=False)\\ntest_accs, train_accs = cross_validate(bc, Xtrain, ytrain, boost=True, seed=None, text=True)\\n\\nprint('')\\nprint(test_accs, np.mean(test_accs), '\\n')\\nprint(train_accs, np.mean(train_accs))\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc = BoostingClassifier(n_its=100, max_depth=2, print_progress=False)\n",
    "test_accs, train_accs = cross_validate(bc, Xtrain, ytrain, boost=True, seed=None, text=True)\n",
    "\n",
    "print('')\n",
    "print(test_accs, np.mean(test_accs), '\\n')\n",
    "print(train_accs, np.mean(train_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Vectorising and converting to tf-idf format (fitting to train data only)...\\npt = process_text()\\nXtrain = pt.fit_transform(Xtrain)\\nXtest = pt.transform(Xtest)\\n\\n## Testing predictions...\\nsvd.fit(Xtrain, ytrain)\\nsvm_pred_test_y = svd.predict(Xtest)\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorising and converting to tf-idf format (fitting to train data only)...\n",
    "pt = process_text()\n",
    "Xtrain = pt.fit_transform(Xtrain)\n",
    "Xtest = pt.transform(Xtest)\n",
    "\n",
    "## Testing predictions...\n",
    "svd.fit(Xtrain, ytrain)\n",
    "svm_pred_test_y = svd.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.linear_model import SGDClassifier\\n#svd = SGDClassifier(tol=None)\\n# Training...\\n#svd.fit(Xtrain, ytrain)\\n# Predicitng...                \\n#y_test_pred = svd.predict(Xtest)\\nprint('Test set accuracy =', accuracy_score(ytest, svm_pred_test_y)*100,'%')\\nsvm_pred_train_y = svd.predict(Xtrain)\\nprint('Train set accuracy =', accuracy_score(ytrain, svm_pred_train_y)*100,'%')\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "#svd = SGDClassifier(tol=None)\n",
    "# Training...\n",
    "#svd.fit(Xtrain, ytrain)\n",
    "# Predicitng...                \n",
    "#y_test_pred = svd.predict(Xtest)\n",
    "print('Test set accuracy =', accuracy_score(ytest, svm_pred_test_y)*100,'%')\n",
    "svm_pred_train_y = svd.predict(Xtrain)\n",
    "print('Train set accuracy =', accuracy_score(ytrain, svm_pred_train_y)*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntree = DecisionTreeClassifier(max_depth=2)\\ntree.fit(Xtrain, ytrain)\\npred_y = tree.predict(Xtest)\\nprint('Test set accuracy =', accuracy_score(ytest, pred_y))\\nprint(pred_y)\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=2)\n",
    "tree.fit(Xtrain, ytrain)\n",
    "pred_y = tree.predict(Xtest)\n",
    "print('Test set accuracy =', accuracy_score(ytest, pred_y))\n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Boosting classifier...\\nbc = BoostingClassifier(n_its=100, max_depth=4, print_progress=True)\\n# Training...\\nbc.fit(Xtrain, ytrain)\\n# Predicitng...                \\nbc_pred_test_y = bc.boost_predict(Xtest)\\nprint('Test set accuracy =', accuracy_score(ytest, bc_pred_test_y))\\nbc_pred_train_y = bc.boost_predict(Xtrain)\\nprint('Train set accuracy =', accuracy_score(ytrain, bc_pred_train_y))\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Boosting classifier...\n",
    "bc = BoostingClassifier(n_its=100, max_depth=4, print_progress=True)\n",
    "# Training...\n",
    "bc.fit(Xtrain, ytrain)\n",
    "# Predicitng...                \n",
    "bc_pred_test_y = bc.boost_predict(Xtest)\n",
    "print('Test set accuracy =', accuracy_score(ytest, bc_pred_test_y))\n",
    "bc_pred_train_y = bc.boost_predict(Xtrain)\n",
    "print('Train set accuracy =', accuracy_score(ytrain, bc_pred_train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "\n",
    "Classifying the movie review dataset using a Support Vector Machine (SVM) with tailored kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Intialising SVM...\\nsvc = SVMClassifier(kernel='sigmoid') # 'rbf'\\n# Extracting and cleaning review data...\\n#X,y = extract_bag_of_words('movie_review_train.csv')\\n# Performing cross-validation...\\n#test_accs, train_accs = cross_validate(svc, X, y, boost=False, seed=None, text=True)\\n\\n# Fitting...\\nsvc.fit_text(Xtrain, ytrain)\\n# Predicitng...  \\ntext_svc_test_pred = svc.predict_text(Xtest)\\n\\n# Printing accuracy...\\nprint('Test set accuracy =', accuracy_score(ytest, text_svc_test_pred))\\ntext_svc_train_pred = svc.predict_text(Xtrain)\\nprint('Train set accuracy =', accuracy_score(ytrain, text_svc_train_pred))\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intialising SVM...\n",
    "svc = SVMClassifier(kernel='sigmoid')\n",
    "# Extracting and cleaning review data...\n",
    "X,y = extract_bag_of_words('movie_review_train.csv')\n",
    "# Performing cross-validation...\n",
    "test_accs, train_accs = cross_validate(svc, X, y, boost=False, seed=None, text=True)\n",
    "\n",
    "# Fitting...\n",
    "svc.fit_text(Xtrain, ytrain)\n",
    "# Predicitng...  \n",
    "text_svc_test_pred = svc.predict_text(Xtest)\n",
    "\n",
    "# Printing accuracy...\n",
    "print('Test set accuracy =', accuracy_score(ytest, text_svc_test_pred))\n",
    "text_svc_train_pred = svc.predict_text(Xtrain)\n",
    "print('Train set accuracy =', accuracy_score(ytrain, text_svc_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spline_kernel():\n",
    "    '''\n",
    "    Defining the histogram intersection kernel.\n",
    "    (VECTORISED)\n",
    "    ========================================\n",
    "    Input\n",
    "    ========================================\n",
    "    X1: 2D numpy array\n",
    "    X2: 2D numpy array\n",
    "    \n",
    "    ========================================\n",
    "    Output\n",
    "    ========================================\n",
    "    Covariance matrix (2D numpy array)\n",
    "    '''\n",
    "    def fit(self, X1, X2):\n",
    "        N1 = X1.shape[0]\n",
    "        N2 = X2.shape[0]\n",
    "        \n",
    "        cov = np.zeros((N1,N2))\n",
    "        for i in range(N1):\n",
    "            for j in range(N2):\n",
    "                X1_i, X2_j = X1[i], X2[j]\n",
    "                \n",
    "                min_xy = np.minimum(X1_i, X2_j)\n",
    "                xy = X1_i * X2_j\n",
    "                x_plus_y = X1_i + X2_j\n",
    "                \n",
    "                arg = 1 + xy + (xy*min_xy) - (0.5*x_plus_y*(min_xy**2)) + ((min_xy**3)/3)\n",
    "                cov[i,j] = np.prod(arg)\n",
    "        \n",
    "        return cov"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
